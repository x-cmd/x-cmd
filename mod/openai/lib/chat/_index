# shellcheck shell=dash disable=2034,3028
# https://platform.openai.com/docs/quickstart?language=curl
# this is a moduel to honor x-bash/chat module

xrc chat
xrc:mod:lib     openai      chat/send chat/exec

# TODO: preparehistory
___x_cmd_openai_chat(){
    local X_help_cmd='___x_cmd help -m openai chat'; help:arg-null:parse
    local op="$1";
    case "$op" in
        request|exec)
            shift; ___x_cmd_openai_chat_"$op" "$@" ;;
        --def-model_)
            shift; ___x_cmd_openai_chat_def_model_ "$@" ;;
        *)  N=openai M="Not support such [subcmd=$op]" log:ret:64
    esac
}

___x_cmd_openai_chat_def_model_(){
    x_="$___X_CMD_OPENAI_DEFAULT_FIRST_MODEL"
}

# TDDO: gemini
___x_cmd_openai_chat_provider___validate(){
    local provider="$1"
    case "$provider" in
        openai|mistral|deepseek|grok|gh|moonshot|zhipu|siliconflow|openrouter|ollama|llmf|lms) return 0 ;;
        *)
            N=openai M="Provider API format must be OpenAI-compatible" log:ret:64
            ;;
    esac
}

___x_cmd_openai_chat_request(){
    local X_help_cmd='___x_cmd help -m openai chat request'; help:arg:parse
    ___x_cmd chat --exec --provider openai "$@"
}

___x_cmd_openai_chat_request___launch(){
    local act_provider="$___X_CMD_OPENAI_CHAT_ACTUAL_PROVIDER"
    local content_dir
    ___x_cmd_readr content_dir
    [ -d "$content_dir" ] || return 1

    ___x_cmd_openai_chat_request___trapexit(){
        ___x_cmd log ":${act_provider}" debug "Remove chat.running file"
        ___x_cmd rmrf "$content_dir/chat.running"
    }

    printf "%s\n" $$ >"$content_dir/chat.running"
    trap '___x_cmd_openai_chat_request___trapexit' EXIT

    {
        local model=""; local is_stream=""; local is_reasoning=""
        ___x_cmd_readr model
        ___x_cmd_readr is_stream
        ___x_cmd_readr is_reasoning

        local request_body_file="$content_dir/${act_provider}.request.body.yml"
        [ -f "$request_body_file" ] || return $?

        local request_function="${___X_CMD_OPENAI_CHAT_REQUEST_ACTUAL_FUNTION:-___x_cmd_openai_chat_request___try}"
        local _exitcode=0
        if [ -z "$___X_CMD_OPENAI_CHAT_REQUEST_NOT_RETRY" ]; then
            ___x_cmd retry --max 2 --interval 3 "$request_function"
        else
            "$request_function"
        fi
        _exitcode="$?"

        if [ "$is_enactnone" != 1 ]; then
            printf "[MODEL-RECV-AT] %s\n" "${EPOCHREALTIME:-"$(date +%s)"}" >> "$XCMD_CHAT_ENACTALL_DRAWFILE"
            printf "[EXITCODE] %s\n" "$_exitcode" >> "$XCMD_CHAT_ENACTALL_DRAWFILE"
            [ "$_exitcode" = 0 ] || printf "[BREAK] %s\n" "$_exitcode" >> "$XCMD_CHAT_ENACTALL_LOGFILE"
        fi

        ___x_cmd_openai_chat_request___trapexit
        [ "$_exitcode" -eq 0 ] || false
    } || {
        ___x_cmd_openai_chat_request___trapexit
        return 1
    }
}

___x_cmd_openai_chat_request___try(){
    local act_provider="$___X_CMD_OPENAI_CHAT_ACTUAL_PROVIDER"
    ___x_cmd log ":${act_provider}" debug "Sending request to $___X_CMD_OPENAI_CHAT_ACTUAL_PROVIDER_NAME server"
    ___x_cmd_chat___exec__confirm_before_request || return $?

    local _md5; _md5="$( < "$request_body_file" ___x_cmd str md5 )"
    local errcode=0

    [ ! -d "$content_dir/chat.response" ] || ___x_cmd rmrf "$content_dir/chat.response"
    {
        request_body_file="$request_body_file" \
        ___x_cmd ccmd "$cache_time" -- \
            ___x_cmd_openai_request_generaxwtecontent "$model" "$_md5"  \
        | provider_name="$act_provider" \
        ___x_cmd cawk -i -E content_dir,is_stream,is_reasoning,is_debug,is_enactnone,XCMD_CHAT_ENACTALL_LOGFILE,XCMD_CHAT_ENACTALL_DRAWFILE \
            -m j/json,j/jiter,j/jcp                                     \
            -f "$___X_CMD_ROOT_MOD/chat/lib/awk/history.awk"            \
            -f "$___X_CMD_ROOT_MOD/chat/lib/awk/util.awk"               \
            -f "$___X_CMD_ROOT_MOD/chat/lib/awk/minion.awk"             \
            -f "$___X_CMD_ROOT_MOD/chat/lib/awk/cres.awk"               \
            -f "$___X_CMD_ROOT_MOD/chat/lib/awk/creq.awk"               \
            -f "$___X_CMD_ROOT_MOD/openai/lib/awk/openai.awk"           \
            -f "$___X_CMD_ROOT_MOD/openai/lib/awk/openai_stream_output_util.awk"    \
            -f "$___X_CMD_ROOT_MOD/openai/lib/awk/handle_response.awk"
    }
    errcode=$?
    case $errcode in
        0)  return 0 ;;
        2)  ___X_CMD_RETRY_ABORT=1; errcode=1 ;;
    esac

    ___x_cmd rmrf "$content_dir/chat.response" "$content_dir/${act_provider}.response.yml"
    ___x_cmd ccmd - ___x_cmd_openai_request_generaxwtecontent "$model" "$_md5"
    return "$errcode"
}

# ___x_cmd openai chat --model gpt4 -n 3 --file a.md -f b.md -f c.md --prompt english ''

# @gpt3 -n 3 -f a.md -f b.md -p en --
# @gpt3 -n 3 -f a.md -f c.jd -p cn --
# @gpt4

# using control command for this like vscode

# ___x_cmd chat start

# ___x_cmd chat start ==> create a new thread
# ___x_cmd chat set 3 ==> set history to 3
# @gpt3 how to understand

# @ws /start
# @ws /set 3

# ___x_cmd chat history cp ''

# @en
# @cn


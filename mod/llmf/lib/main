# shellcheck shell=dash

___x_cmd log init llmf
___X_CMD_LLMF_DEFAULT_FIRST_MODEL="llava/v1.5-7b/q4_k.gguf" # "tinyllama/1.1b-chat-v1.0/q4_k_m.gguf"
___X_CMD_LLMF_DEFAULT_ENDPOINT="http://localhost:8080"
xrc:mod:lib     pkg envvar
xrc:mod:lib     llmf cfg model/_index cli serve nv chat/_index

! ___x_cmd_is_suitable_advise_env || xrc:mod:lib    llmf advise

___x_cmd_llmf___main(){
    [ "$#" -gt 0 ]      ||   set - nv

    local op="$1";      shift
    case "$op" in
        -h|--help)      ___x_cmd help -m llmf              ;;
        model)          ___x_cmd_llmf_model         "$@" ;;

        serve)          ___x_cmd_llmf_serve         "$@" ;;
        cli)            ___x_cmd_llmf_cli           "$@" ;;
        chat)           ___x_cmd_llmf_chat          "$@" ;;
        nv)             ___x_cmd_llmf_nv            "$@" ;;

        tokenize)       ___x_cmd_llmf_tokenize      "$@" ;;
        --hascmd)       ___x_cmd_llmf___hascmd      "$@" ;;
        --cfg)          ___x_cmd_llmf_cfg           "$@" ;;
        --cur)          ___x_cmd_llmf_cur           "$@" ;;
        init|--init)    ___x_cmd_llmf_init          "$@" ;;

        --)             ___x_cmd_llmf_cli           "$@" ;;
        *)              ___x_cmd_llmf_cli "$op"     "$@" ;;
    esac
}

___x_cmd_llmf___llamafile_exec(){
    [ "$#" -gt 0 ]      ||   set - --help
    ___x_cmd cosmo llamafile-0.8.4 "$@"
}


___x_cmd_llmf___exec(){
    local model="$1";    shift
    case "$model" in
        *.llamafile)    ___x_cmd cosmo --arun                   "$model" "$@" ;;
        *.gguf)         ___x_cmd cosmo llamafile-0.8.4 --model  "$model" "$@" ;;
    esac
}

___x_cmd_llmf___hascmd(){
    xrc cosmo
    local x_; ___x_cmd_cosmo___which_ llamafile-0.8.4
}

___x_cmd_llmf_tokenize(){
    ___x_cmd cosmo llamafile-tokenize "$@"
}
